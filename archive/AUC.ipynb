{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils import data\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "#from visdom import Visdom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data_process_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split_h(data, mask):  # data == tensor   mask == torch    b t input\n",
    "    # sample, timestep, feature\n",
    "    train_data = data[2682:, :, 3:53]\n",
    "    train_mask = mask[2682:, :, 3:53]\n",
    "    valid_data = data[1341:2682, :, 3:53]\n",
    "    valid_mask = mask[1341:2682, :, 3:53]\n",
    "    test_data = data[:1341, :, 3:53]\n",
    "    test_mask = mask[:1341, :, 3:53]\n",
    "\n",
    "    # TODO\n",
    "    # 舍弃第36个特征\n",
    "    train_data = train_data[:, :, torch.arange(train_data.size(2)) != 36]\n",
    "    train_mask = train_mask[:, :, torch.arange(train_mask.size(2)) != 36]\n",
    "    valid_data = valid_data[:, :, torch.arange(valid_data.size(2)) != 36]\n",
    "    valid_mask = valid_mask[:, :, torch.arange(valid_mask.size(2)) != 36]\n",
    "    test_data = test_data[:, :, torch.arange(test_data.size(2)) != 36]\n",
    "    test_mask = test_mask[:, :, torch.arange(test_mask.size(2)) != 36]\n",
    "\n",
    "    return train_data, train_mask, valid_data, valid_mask, test_data, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "      #'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, dataset, mask, valid_data, valid_mask, test_data, test_mask):\n",
    "        #'Initialization'\n",
    "        #self.X = torch.load(path)\n",
    "        #self.X = pickle.load(open(path, 'rb'))\n",
    "        self.X = dataset\n",
    "        self.M = mask\n",
    "        self.vx = valid_data\n",
    "        self.vm = valid_mask\n",
    "        self.tx = test_data\n",
    "        self.tm = test_mask\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        #'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #'Generates one sample of data'\n",
    "        # Select sample\n",
    "        return self.X[index], self.M[index], self.vx[index], self.vm[index], self.tx[index], self.tm[index]\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data, mask):\n",
    "        self.data = data\n",
    "        self.mask = mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.data[index], self.mask[index]\n",
    "    \n",
    "class VaildDataset(Dataset):\n",
    "    def __init__(self, data, mask):\n",
    "        self.data = data\n",
    "        self.mask = mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.data[index], self.mask[index]\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data, mask):\n",
    "        self.data = data\n",
    "        self.mask = mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.data[index], self.mask[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,hidden_size=64, num_layers=2, dropout=0.2,input_size=49):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_size,     # rnn hidden unit\n",
    "            num_layers=self.num_layers,       # number of rnn layer\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.drop_layer = nn.Dropout(p=self.dropout)\n",
    "        self.out = nn.Linear(self.hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x, h_state=None, is_train=True, gen_length=None):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, hidden_size)\n",
    "        batch = x.shape[0]\n",
    "\n",
    "        if is_train==False:\n",
    "            r_output = torch.zeros(gen_length, batch, self.input_size).cuda()  #   t b input_size\n",
    "            r_out = x.permute(1,0,2)[23] #->  b hidden_size\n",
    "            r_output[:23,:,:] = x.permute(1,0,2)[:23,:,:]\n",
    "            r_out = r_out.view(batch, 1, -1) #->  b 1 hidden_size\n",
    "            for i in range(24-1,gen_length-1):\n",
    "                r_out, h_state = self.rnn(r_out, h_state) #r_out  b 1 hidden_size\n",
    "                r_out = self.drop_layer(r_out)\n",
    "                r_out = self.out(r_out) #-> b t inputsize\n",
    "                r_output[i+1]= r_out.permute(1,0,2)[0]\n",
    "            return r_output.permute(1,0,2), h_state\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        #return r_out, h_state\n",
    "        r_out = self.drop_layer(r_out)\n",
    "        r_out = self.out(r_out) #-> b t inputsize        \n",
    "        return r_out, h_state\n",
    "\n",
    "# In[6]:\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,hidden_size=64, num_layers=2, dropout=0.2,input_size=49):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_size,     # rnn hidden unit\n",
    "            num_layers=self.num_layers,       # number of rnn layer\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.drop_layer = nn.Dropout(p=self.dropout)\n",
    "        self.out = nn.Linear(self.hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x, h_state=None, is_train=True, gen_length=None):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, hidden_size)\n",
    "        batch = x.shape[0]\n",
    "\n",
    "        if is_train==False:\n",
    "            r_output = torch.zeros(gen_length, batch, self.input_size).cuda()  #   t b input_size\n",
    "            r_out = x.permute(1,0,2)[23] #->  b hidden_size\n",
    "            r_output[:23,:,:] = x.permute(1,0,2)[:23,:,:]\n",
    "            r_out = r_out.view(batch, 1, -1) #->  b 1 hidden_size\n",
    "            for i in range(24-1,gen_length-1):\n",
    "                r_out, h_state = self.rnn(r_out, h_state) #r_out  b 1 hidden_size\n",
    "                r_out = self.drop_layer(r_out)\n",
    "                r_out = self.out(r_out) #-> b t inputsize\n",
    "                r_output[i+1]= r_out.permute(1,0,2)[0]\n",
    "            return r_output.permute(1,0,2), h_state\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        #return r_out, h_state\n",
    "        r_out = self.drop_layer(r_out)\n",
    "        r_out = self.out(r_out) #-> b t inputsize        \n",
    "        \n",
    "        return r_out, h_state\n",
    "\n",
    "# In[6]:\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self,hidden_size=64, num_layers=2, dropout=0.2,input_size=49):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_size,     # rnn hidden unit\n",
    "            num_layers=self.num_layers,       # number of rnn layer\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.drop_layer = nn.Dropout(p=self.dropout)\n",
    "        self.out = nn.Linear(self.hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x, h_state=None, is_train=True, gen_length=None):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size)\n",
    "        # r_out (batch, time_step, hidden_size)\n",
    "        batch = x.shape[0]\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        #return r_out, h_state\n",
    "        r_out = self.drop_layer(r_out)\n",
    "        r_out = self.out(r_out) #-> b t inputsize\n",
    "        if is_train==False:\n",
    "            r_output = torch.zeros(gen_length, batch, self.input_size).cuda()  #   t b input_size\n",
    "            r_out = x.permute(1,0,2)[23] #->  b hidden_size\n",
    "            r_output[:23,:,:] = x.permute(1,0,2)[:23,:,:]\n",
    "            r_out = r_out.view(batch, 1, -1) #->  b 1 hidden_size\n",
    "            for i in range(23,gen_length-1):\n",
    "                r_out, h_state = self.rnn(r_out, h_state) #r_out  b 1 hidden_size\n",
    "                r_out = self.drop_layer(r_out)\n",
    "                r_out = self.out(r_out) #-> b t inputsize\n",
    "                r_output[i+1]= r_out.permute(1,0,2)[0]\n",
    "            return r_output.permute(1,0,2), h_state\n",
    "        return r_out, h_state\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, data_x, data_y, mask, loss_fn):  # x (batch, time_step, input_size)\n",
    "    batch = data_y.shape[0]\n",
    "    length = data_y.shape[1]\n",
    "    length=48\n",
    "    Pre_y = torch.zeros(length, batch, data_y.shape[2]).cuda()\n",
    "    Pre_y, _ = model(data_y, is_train=False, gen_length=length) # b t input\n",
    "    Pre_y = torch.mul(Pre_y,mask[:,0:48,:])\n",
    "    eval_loss_r = loss_fn(data_y.permute(2,1,0)[:47,24:length,:], Pre_y.permute(2,1,0)[:47,24:length,:])   # input t b\n",
    "    eval_loss_t = loss_fn(data_y.permute(2,1,0)[47:,24:length,:], Pre_y.permute(2,1,0)[47:,24:length,:])\n",
    "    return eval_loss_r, eval_loss_t, data_y[:,24:length,:], Pre_y[:,24:length,:]  # b t input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    pkl_file = open(path, 'rb')\n",
    "    return pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10736, 74, 49])\n",
      "torch.Size([10736, 74, 49])\n"
     ]
    }
   ],
   "source": [
    "params = {'batch_size': 64,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 0}\n",
    "dataset, mask = read_dataset('../datasets/normalized_dataset_new.pkl')\n",
    "train_data, train_mask, valid_data, valid_mask, test_data, test_mask = dataset_split_h(dataset, mask) # b t input\n",
    "print(train_data.shape)\n",
    "print(train_mask.shape)\n",
    "\n",
    "train_dataset = TrainDataset(train_data, train_mask)\n",
    "vaild_dataset = VaildDataset(valid_data, valid_mask)\n",
    "test_dataset = TestDataset(test_data, test_mask)\n",
    "\n",
    "train_iterator= data.DataLoader(train_dataset, **params)\n",
    "vaild_iterator=data.DataLoader(vaild_dataset,**params)\n",
    "test_iterator=data.DataLoader(test_dataset,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(128, 1, 0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r=LSTM(128, 1, 0,47).cuda()\n",
    "model_t=LSTM(128, 1, 0,2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(Y, Y_pred):  # b t input\n",
    "    num = 0\n",
    "    for y in Y:\n",
    "        num += y.shape[0]   #batch number\n",
    "    ground_truth = Y[0]\n",
    "    predict = Y_pred[0]\n",
    "    for i, y in enumerate(Y[1:]):\n",
    "        ground_truth = torch.cat((ground_truth, y),0)\n",
    "        predict = torch.cat((predict, Y_pred[i+1]),0)\n",
    "    g_r = ground_truth.permute(2,1,0)[:47]\n",
    "    g_t = ground_truth.permute(2,1,0)[47:]\n",
    "    y_r = predict.permute(2,1,0)[:47]\n",
    "    y_t = predict.permute(2,1,0)[47:]\n",
    "    g_r = torch.unsqueeze(g_r, 0)\n",
    "    g_t = torch.unsqueeze(g_t, 0)\n",
    "    y_r = torch.unsqueeze(y_r, 0)\n",
    "    y_t = torch.unsqueeze(y_t, 0)\n",
    "    l2_r = F.mse_loss(g_r,y_r)\n",
    "    l2_t = F.mse_loss(g_t,y_t)\n",
    "    l1_r = F.l1_loss(g_r,y_r)\n",
    "    l1_t = F.l1_loss(g_t,y_t)\n",
    "    geometric_mean = np.exp(np.log([l1_t.item(), l1_r.item(), l2_t.item(), l2_r.item()]).mean())\n",
    "    return l1_r.item(), l1_t.item(), l2_r.item(), l2_t.item(),geometric_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"../model_test/lstm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r=torch.load(\"../model_test/LSTM_r_t_[128, 256, 512]_[1, 2]_[0.2, 0.5]_[64, 128, 256]r.model\")\n",
    "model_t=torch.load(\"../model_test/LSTM_r_t_[128, 256, 512]_[1, 2]_[0.2, 0.5]_[64, 128, 256]t.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rt(model, data_x, data_y, mask, loss_fn):  # x (batch, time_step, input_size)\n",
    "    batch = data_x.shape[0]\n",
    "    length = data_y.shape[1]\n",
    "    length = 48\n",
    "    Pre_y, _ = model(data_x, is_train=False, gen_length=length) # b t input\n",
    "    Pre_y = torch.mul(Pre_y,mask[:,:48,-2:])\n",
    "    eval_loss_r = loss_fn(data_y[:,:length,:], Pre_y)\n",
    "    return eval_loss_r, data_y[:,24:length,:], Pre_y[:,24:length,:]  # b t input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:691: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:925.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "Y = []\n",
    "Y_pred = []\n",
    "TN=0\n",
    "TP=0\n",
    "FN=0\n",
    "FP=0\n",
    "pred_t_n=[]\n",
    "t_n=[]\n",
    "for b, batch in enumerate(test_iterator):\n",
    "    data_z, mask  = batch\n",
    "    # data_xy = torch.cat((data_x,data_y), 1)\n",
    "    if torch.cuda.is_available():\n",
    "        # data_xy = data_xy.cuda()\n",
    "        data_z = data_z.cuda()\n",
    "        mask = mask.cuda()\n",
    "    eval_loss_r, eval_loss_t, y, y_pred = evaluate(model, data_z, data_z, mask, loss_fn)\n",
    "    # eval_loss_t, y, y_pred = evaluate_rt(model_t, data_z[:,:,47:], data_z[:,:,47:], mask, loss_fn)\n",
    "    for i in range(batch[0].size(0)):\n",
    "        treatment=data_z[i, 24:48, -2:].nonzero()\n",
    "        pred_t=y_pred[i,:,-2:]>=0.05\n",
    "        treatment_pred=pred_t[:,-2:].nonzero()\n",
    "        t_n.append( 0 if treatment.shape[0]==0 else 1)\n",
    "        pred_t_n.append(0 if treatment_pred.shape[0]==0 else 1)\n",
    "        if treatment.shape[0]==0 and treatment_pred.shape[0]==0:\n",
    "            TN=TN+1\n",
    "        if treatment.shape[0]!=0 and treatment_pred.shape[0]!=0:\n",
    "            TP=TP+1  \n",
    "        if treatment.shape[0]==0 and treatment_pred.shape[0]!=0:\n",
    "            FP=FP+1   \n",
    "        if treatment.shape[0]!=0 and treatment_pred.shape[0]==0:\n",
    "            FN=FN+1    \n",
    "    Y.append(y)\n",
    "    Y_pred.append(y_pred)\n",
    "l1_r,l1_t,l2_r,l2_t,geo_mean = RMSE(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 327, 645, 19)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN,TP,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.9450867052023122      precision:  0.33641975308641975\n"
     ]
    }
   ],
   "source": [
    "Recall=TP/(TP+FN)\n",
    "Precision=TP/(TP+FP)\n",
    "print(\"recall:\",str(Recall),\"     precision: \",str(Precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "auc=metrics.roc_auc_score(t_n,pred_t_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6484227495860807"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
